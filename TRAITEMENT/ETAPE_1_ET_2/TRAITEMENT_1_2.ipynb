{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d250bc2-d286-412b-823c-b8170a645e15",
   "metadata": {},
   "source": [
    "# I. Configuration du projet et initialisation de l'environnement\n",
    "\n",
    "### 1.1. Description technique\n",
    "Ce module constitue le socle structurel du projet. Il assure la centralisation des paramètres de flux de données via un dictionnaire de configuration (`config`) et l'automatisation de l'arborescence de travail. \n",
    "\n",
    "Le script orchestre plusieurs dimensions critiques :\n",
    "* **Gestion des Répertoires** : Utilisation de la bibliothèque `pathlib` pour garantir l'agnosticisme du système d'exploitation (Windows/Linux) dans le traitement des chemins.\n",
    "* **Référentiel d'Identité** : Définition d'une clé composite (`CHAMPS_ID`) incluant le champ `DALLE`, permettant une traçabilité géographique fine et un partitionnement des calculs.\n",
    "* **Paramétrage Multi-sources** : Centralisation des millésimes (2017-2024) et des chemins d'accès pour les cinq sources de données majeures (OSO, RPG standard et complet, COSIA, BD FORÊT et BD TOPO).\n",
    "* **Déploiement Automatisé** : Création dynamique des sous-répertoires de sortie pour assurer l'organisation des données intermédiaires (CSV).\n",
    "\n",
    "### 1.2. Justification méthodologique\n",
    "L'adoption de cette architecture centralisée répond à trois impératifs:\n",
    "\n",
    "1. **Reproductibilité** : La séparation stricte entre les répertoires de données sources (`ENTRER`) et les sorties (`SORTIE`) permet de rejouer l'intégralité du pipeline sur d'autres territoires sans modifier la logique algorithmique.\n",
    "2. **Optimisation des Performances** : L'intégration du champ `DALLE` dès l'initialisation prépare le système à des traitements par tuiles. Cette approche est indispensable pour prévenir les dépassements de mémoire vive lors des opérations d'intersection géométrique sur des jeux de données massifs (Big Data spatial).\n",
    "3. **Harmonisation Temporelle** : En définissant les listes d'années par source dans une configuration unique, on garantit la cohérence chronologique lors des phases ultérieures de croisement et de calcul de stabilité de l'occupation du sol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c58c204c-f53a-4432-abb8-60779ed00e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environnement initialisé : Prêt pour le traitement multi-sources avec gestion des dalles.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# 1. DÉFINITION DES RÉPERTOIRES RACINES\n",
    "# =============================================================================\n",
    "DIR_ENTREE_SIG   = Path(r'C:/Users/liege/Desktop/GEOLAB/ENTRER')\n",
    "DIR_ENTREE_NOMEN = Path(r'C:/Users/liege/Desktop/GEOLAB/NOMENCLATURE')\n",
    "DIR_SORTIE       = Path(r'C:/Users/liege/Desktop/GEOLAB/SORTIE')\n",
    "\n",
    "# =============================================================================\n",
    "# 2. CONFIGURATION GÉNÉRALE DES FLUX DE DONNÉES\n",
    "# =============================================================================\n",
    "config = {\n",
    "    # Ajout du champ \"DALLE\" pour assurer la traçabilité géographique par secteur\n",
    "    'CHAMPS_ID': [\"CODE_DEP\", \"NOM_COM\", \"CODE_COM\", \"SECTION\", \"NUMERO\", \"DALLE\"],\n",
    "    \n",
    "    # Référentiels spatiaux de base\n",
    "    'ZONE_PATH'     : DIR_ENTREE_SIG / \"ZONE.gpkg\",\n",
    "    'CADASTRE_PATH' : DIR_ENTREE_SIG / \"CADASTRE.gpkg\",\n",
    "    \n",
    "    # Configuration OSO (Occupation du Sol - CESBIO)\n",
    "    'OSO_GPKG'       : DIR_ENTREE_SIG / \"OSO_2018_2023_ALL.gpkg\",\n",
    "    'NOMEN_OSO_CSV'  : DIR_ENTREE_NOMEN / \"Nomenclature_OSO.csv\",\n",
    "    'ANNEES_OSO'     : [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\"],\n",
    "    'DOSSIER_CSV_OSO': DIR_SORTIE / \"CSV_OSO\",\n",
    "    \n",
    "    # Configuration RPG (Fusion des sources Standard et Complet)\n",
    "    'RPG_GPKG'        : DIR_ENTREE_SIG / \"RPG_2018_2024_ALL.gpkg\",\n",
    "    'RPG_COMP_GPKG'   : DIR_ENTREE_SIG / \"RPG_COMPLETE_2018_2022_ALL.gpkg\",\n",
    "    'ANNEES_RPG'      : [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\", \"2023\", \"2024\"],\n",
    "    'ANNEES_RPG_COMP' : [\"2018\", \"2019\", \"2020\", \"2021\", \"2022\"],\n",
    "    'DIR_CSV_RPG_FUS' : DIR_SORTIE / \"CSV_RPG_FUSION\",\n",
    "\n",
    "    # Configuration COSIA\n",
    "    'COSIA_GPKG'       : DIR_ENTREE_SIG / \"COSIA_2018_2024_ALL.gpkg\",\n",
    "    'NOMEN_COSIA_CSV'  : DIR_ENTREE_NOMEN / \"Nomenclature_COSIA.csv\",\n",
    "    'ANNEES_COSIA'     : [\"2018\", \"2021\", \"2024\"],\n",
    "    'DOSSIER_CSV_COSIA': DIR_SORTIE / \"CSV_COSIA\",\n",
    "\n",
    "    # Configuration BD FORÊT (IGN)\n",
    "    'FORET_PATH'       : DIR_ENTREE_SIG / \"FORMATION_VEGETALE_BD_FORET_2017.gpkg\",\n",
    "    'DOSSIER_CSV_FORET': DIR_SORTIE / \"CSV_FORET\",\n",
    "    \n",
    "    # Configuration BD TOPO (IGN)\n",
    "    'BDTOPO_PATH'       : DIR_ENTREE_SIG / \"BDTOPO_2025.gpkg\",\n",
    "    'DOSSIER_CSV_BDTOPO': DIR_SORTIE / \"CSV_BDTOPO\",\n",
    "    'COUCHES_BDTOPO'    : [\n",
    "        \"BATIMENT\", \"CIMETIERE\", \"CONSTRUCTION_SURFACIQUE\", \n",
    "        \"SURFACE_HYDROGRAPHIQUE\", \"RESERVOIR\", \"TERRAIN_DE_SPORT\", \n",
    "        \"ZONE_D_ESTRAN\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 3. INITIALISATION DE L'ARBORESCENCE DE SORTIE\n",
    "# =============================================================================\n",
    "dossiers_requis = [\n",
    "    DIR_SORTIE,\n",
    "    config['DOSSIER_CSV_OSO'],\n",
    "    config['DIR_CSV_RPG_FUS'],\n",
    "    config['DOSSIER_CSV_COSIA'],\n",
    "    config['DOSSIER_CSV_FORET'],\n",
    "    config['DOSSIER_CSV_BDTOPO']\n",
    "]\n",
    "\n",
    "for dossier in dossiers_requis:\n",
    "    dossier.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Environnement initialisé : Prêt pour le traitement multi-sources avec gestion des dalles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147c9e86-3e80-4eb9-9ca3-a909be545e88",
   "metadata": {},
   "source": [
    "# II. Extraction du référentiel parcellaire et indexation spatiale\n",
    "\n",
    "### 2.1. Description technique\n",
    "Cette étape constitue le socle géographique de l'étude par la création d'un référentiel foncier précis. Le processus s'articule autour de deux opérations majeures :\n",
    "* **Filtrage spatial à la source** : Chargement ciblé du cadastre national à l'aide d'un masque (`mask`) basé sur l'emprise d'étude. L'utilisation du moteur `pyogrio` garantit une lecture performante des géométries complexes.\n",
    "* **Indexation par jointure spatiale (`SJoin`)** : Enrichissement de la donnée parcellaire par l'injection de l'attribut `DALLE`. Cette opération de voisinage (`predicate='intersects'`) permet de localiser chaque parcelle dans le système de partitionnement du projet.\n",
    "\n",
    "### 2.2. Justification de l'approche\n",
    "Le recours à cette méthode d'extraction et d'indexation répond à des impératifs d'optimisation et de traçabilité :\n",
    "\n",
    "1. **Intégrité des données** : L'extraction par masque garantit que seules les parcelles intersectant la zone d'intérêt sont conservées, éliminant ainsi les bruits de fond et les données superflues hors périmètre d'étude.\n",
    "2. **Gestion de la volumétrie (Tiling)** : L'attribution d'un numéro de `DALLE` à chaque parcelle est stratégique. Elle permettrait de fragmenter les futurs calculs statistiques par secteur, évitant ainsi la saturation de la mémoire vive sur des analyses multi-annuelles.\n",
    "3. **Traçabilité géographique** : Cette indexation offre une double lecture du territoire : administrative (via les codes parcelles) et opérationnelle (via le découpage par dalles), indispensable pour le contrôle qualité des traitements SIG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "674b8d7f-ab4a-46d3-aa6b-b11609aedf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de l'emprise et extraction du cadastre...\n",
      "Référentiel prêt : 6792 parcelles chargées avec leur numéro de DALLE.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 2. EXTRACTION DU RÉFÉRENTIEL PARCELLAIRE ET JOINTURE DALLE\n",
    "# =============================================================================\n",
    "# Chargement de l'emprise d'étude et découpage du cadastre national.\n",
    "\n",
    "print(\"Chargement de l'emprise et extraction du cadastre...\")\n",
    "\n",
    "gdf_zone = gpd.read_file(config['ZONE_PATH'])\n",
    "\n",
    "# Chargement du cadastre avec le masque de la zone\n",
    "gdf_parcelles = gpd.read_file(\n",
    "    config['CADASTRE_PATH'], \n",
    "    mask=gdf_zone,\n",
    "    engine='pyogrio'\n",
    ")\n",
    "\n",
    "# --- AJOUT DE LA JOINTURE POUR LE NUMÉRO DE DALLE ---\n",
    "# On récupère l'attribut 'DALLE' de la zone pour l'injecter dans les parcelles\n",
    "gdf_parcelles = gpd.sjoin(gdf_parcelles, gdf_zone[['DALLE', 'geometry']], how='left', predicate='intersects')\n",
    "\n",
    "# Nettoyage de l'index de jointure (colonne technique inutile)\n",
    "if 'index_right' in gdf_parcelles.columns:\n",
    "    gdf_parcelles = gdf_parcelles.drop(columns='index_right')\n",
    "# ----------------------------------------------------\n",
    "\n",
    "print(f\"Référentiel prêt : {len(gdf_parcelles)} parcelles chargées avec leur numéro de DALLE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e9417-ecff-4039-90b8-3feca697841f",
   "metadata": {},
   "source": [
    "# III. Analyse multi-annuelle et synthèse de l'occupation du sol (pour OSO)\n",
    "\n",
    "### 3.1. Module 1 : Analyse statistique annuelle par unité foncière\n",
    "Ce premier module opère une synthèse annuelle de l'occupation du sol. Le processus est structuré autour de l'intersection géométrique entre le cadastre et les données OSO.\n",
    "* **Intersection spatiale (`Overlay`)** : Découpage des couches vectorielles annuelles par le référentiel parcellaire. Chaque fragment résultant permet de calculer précisément la surface occupée par chaque classe au sein de la parcelle.\n",
    "* **Calcul de dominance et fiabilité** : Pour chaque millésime, le script identifie la classe majoritaire et calcule sa **probabilité pondérée** :\n",
    "  $$PROBA\\_DOMINANTE = \\frac{\\sum (Surface \\cap \\times Confidence)}{\\sum Surface \\cap}$$\n",
    "  Cette méthode garantit que la confiance affichée est proportionnelle à la surface réelle occupée par la classe, minimisant ainsi l'impact des artefacts de classification en bordure de parcelle.\n",
    "\n",
    "\n",
    "\n",
    "### 3.2. Module 2 : Synthèse temporelle et arbitrage statistique\n",
    "L'objectif de ce second module est de dégager une tendance de long terme (2018-2023) pour chaque parcelle en s'affranchissant des erreurs ponctuelles de détection satellite ou IA.\n",
    "* **Consolidation chronologique** : Fusion des résultats annuels via l'identifiant unique `PL` pour créer un historique complet de l'occupation du sol par unité foncière.\n",
    "* **Logique de décision par majorité fréquentielle** : Le système analyse l'occurrence de chaque classe sur 6 ans. La classe la plus fréquente est élue comme `DOM_OSO_FINALE`.\n",
    "* **Arbitrage par récence** : En cas d'égalité de fréquence (ex: 3 ans en \"Forêt\" et 3 ans en \"Agriculture\"), le script applique une règle de **chronologie inverse**. Il interroge les données en partant de l'année la plus récente (2023) pour trancher en faveur de l'état le plus actuel du terrain.\n",
    "* **Indicateur de stabilité et fiabilité moyenne** :\n",
    "    * **Score de stabilité** : Pourcentage de présence de la classe élue sur la période étudiée.\n",
    "    * **Proba moyenne** : Moyenne des indices de confiance calculés lors du Module 1, uniquement pour les années où la classe élue était présente.\n",
    "\n",
    "\n",
    "\n",
    "### 3.3. Justification Méthodologique\n",
    "Cette approche en deux étapes permet de produire un diagnostic territorial **résilient**. En combinant la précision spatiale annuelle (Module 1) et la stabilité temporelle (Module 2), le modèle final est capable de distinguer les changements réels d'occupation du sol des simples \"bruits\" de capteurs ou de variations saisonnières."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87ee52d9-17fe-4658-b81d-c0095d3c7d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Analyse spatiale OSO - Millesime 2018\n",
      "SUCCES : Export millesime 2018 genere.\n",
      "INFO : Analyse spatiale OSO - Millesime 2019\n",
      "SUCCES : Export millesime 2019 genere.\n",
      "INFO : Analyse spatiale OSO - Millesime 2020\n",
      "SUCCES : Export millesime 2020 genere.\n",
      "INFO : Analyse spatiale OSO - Millesime 2021\n",
      "SUCCES : Export millesime 2021 genere.\n",
      "INFO : Analyse spatiale OSO - Millesime 2022\n",
      "SUCCES : Export millesime 2022 genere.\n",
      "INFO : Analyse spatiale OSO - Millesime 2023\n",
      "SUCCES : Export millesime 2023 genere.\n",
      "TRAITEMENT OSO TERMINE.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE : ANALYSE STATISTIQUE DE L'OCCUPATION DU SOL (OSO)\n",
    "# =============================================================================\n",
    "# Ce module realise l'intersection spatiale entre le referentiel parcellaire \n",
    "# et les couches annuelles OSO. Il genere un identifiant unique 'PL' par \n",
    "# concatenation textuelle directe des composantes cadastrales.\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- 1. PREPARATION DES REFERENTIELS ET NOMENCLATURES ---\n",
    "classes_list = [\"ARTIFICIALISE\", \"AGRICULTURE\", \"NATURE\", \"BOIS\", \"EAU\", \"AUTRE\"]\n",
    "noms_classes_map = {\"1\":\"ARTIFICIALISE\", \"2\":\"AGRICULTURE\", \"4\":\"NATURE\", \"3\":\"BOIS\", \"5\":\"EAU\", \"0\":\"AUTRE\"}\n",
    "\n",
    "df_nomen = pd.read_csv(config['NOMEN_OSO_CSV'], sep=';', encoding='utf-8-sig')\n",
    "mapping_oso = {str(row['CODE_SOURCE']).strip(): noms_classes_map.get(str(row['CLASSE_6']).strip(), \"AUTRE\") \n",
    "               for _, row in df_nomen.iterrows()}\n",
    "\n",
    "# --- 2. TRAITEMENT ANALYTIQUE ANNUEL ---\n",
    "for annee in config['ANNEES_OSO']:\n",
    "    print(f\"INFO : Analyse spatiale OSO - Millesime {annee}\")\n",
    "    \n",
    "    lyr_oso = None\n",
    "    # Recherche itérative de la couche selon les variantes de nommage possibles\n",
    "    for layer_name in [f\"OSO_VECTEUR_{annee}\", f\"OSO_{annee}\", f\"VECTEUR_{annee}\"]:\n",
    "        try:\n",
    "            lyr_oso = gpd.read_file(config['OSO_GPKG'], layer=layer_name, mask=gdf_parcelles, engine='pyogrio')\n",
    "            if lyr_oso.crs != gdf_parcelles.crs:\n",
    "                lyr_oso = lyr_oso.to_crs(gdf_parcelles.crs)\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if lyr_oso is None:\n",
    "        print(f\"ERREUR : Couche OSO {annee} introuvable dans le GeoPackage.\")\n",
    "        continue\n",
    "\n",
    "    # Operation d'intersection geometrique\n",
    "    inter = gpd.overlay(lyr_oso, gdf_parcelles, how='intersection')\n",
    "    if inter.empty:\n",
    "        continue\n",
    "    \n",
    "    # Calcul des surfaces et identification des attributs sources\n",
    "    inter['S_INT'] = inter.geometry.area\n",
    "    f_classe = \"Classe\" if \"Classe\" in inter.columns else \"CLASSE\"\n",
    "    f_conf = next((f for f in inter.columns if f.lower() == \"confidence\"), None)\n",
    "    inter['CLASSE_NOM'] = inter[f_classe].astype(str).str.strip().map(mapping_oso).fillna(\"AUTRE\")\n",
    "\n",
    "    # Agregation statistique par unite fonciere\n",
    "    stats = inter.groupby(config['CHAMPS_ID'] + ['CLASSE_NOM']).agg({\n",
    "        'S_INT': 'sum',\n",
    "        f_conf: lambda x: (x * inter.loc[x.index, 'S_INT']).sum() if f_conf else 0\n",
    "    }).reset_index()\n",
    "\n",
    "    # Pivotage des donnees pour obtenir une structure par ligne de parcelle\n",
    "    df_res = stats.pivot_table(index=config['CHAMPS_ID'], columns='CLASSE_NOM', values='S_INT', aggfunc='sum').fillna(0)\n",
    "    \n",
    "    for cl in classes_list:\n",
    "        if cl not in df_res.columns: df_res[cl] = 0.0\n",
    "\n",
    "    # Calcul des indicateurs de synthese avec precision a 3 decimales\n",
    "    df_res['SURF_TOTALE_M2'] = df_res[classes_list].sum(axis=1).round(3)\n",
    "    for cl in classes_list:\n",
    "        df_res[f\"{cl}_%\"] = ((df_res[cl] / df_res['SURF_TOTALE_M2']) * 100).round(3)\n",
    "\n",
    "    df_res['DOMINANTE'] = df_res[classes_list].idxmax(axis=1)\n",
    "\n",
    "    # Calcul de la fiabilite ponderee sur la classe dominante\n",
    "    conf_pivot = stats.pivot_table(index=config['CHAMPS_ID'], columns='CLASSE_NOM', values=f_conf, aggfunc='sum').fillna(0)\n",
    "    def get_proba(row):\n",
    "        dom = row['DOMINANTE']\n",
    "        s_dom = row[dom]\n",
    "        cumul = conf_pivot.loc[row.name, dom] if row.name in conf_pivot.index else 0\n",
    "        return round(cumul / s_dom, 1) if s_dom > 0 else 0\n",
    "\n",
    "    df_res['PROBA_DOMINANTE'] = df_res.apply(get_proba, axis=1)\n",
    "\n",
    "    # --- 3. GENERATION DU CHAMP 'PL' (CONCATENATION SIMPLE) ---\n",
    "    df_res = df_res.reset_index()\n",
    "    \n",
    "    # Concaténation des 4 champs racines sans préfixe intermédiaire\n",
    "    # Le passage en .astype(str) garantit le maintien du format texte\n",
    "    df_res['PL'] = (\n",
    "        df_res['CODE_DEP'].astype(str).str.strip() + \n",
    "        df_res['CODE_COM'].astype(str).str.strip() + \n",
    "        df_res['SECTION'].astype(str).str.strip() + \n",
    "        df_res['NUMERO'].astype(str).str.strip()\n",
    "    )\n",
    "\n",
    "    # --- 4. EXPORTATION DES RESULTATS ---\n",
    "    output_csv = config['DOSSIER_CSV_OSO'] / f\"STAT_PARCELLES_OSO_{annee}.csv\"\n",
    "    cols_finales = ['PL'] + config['CHAMPS_ID'] + [f\"{cl}_%\" for cl in classes_list] + \\\n",
    "                   [\"DOMINANTE\", \"PROBA_DOMINANTE\", \"SURF_TOTALE_M2\"]\n",
    "    \n",
    "    df_res[cols_finales].to_csv(output_csv, sep=';', index=False, encoding='utf-8')\n",
    "    print(f\"SUCCES : Export millesime {annee} genere.\")\n",
    "\n",
    "print(\"TRAITEMENT OSO TERMINE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b70bde6-b5b2-4fe1-af0b-aee36872b719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Phase de lecture des fichiers annuels OSO...\n",
      "INFO : Calcul des frequences et arbitrage par chronologie inverse...\n",
      "INFO : Synthese terminee. Resultats : SYNTHESE_OSO_FREQUENCE.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE : SYNTHESE TEMPORELLE ET ARBITRAGE OSO\n",
    "# =============================================================================\n",
    "# Analyse de la stabilite temporelle des classes dominantes par parcelle.\n",
    "# Application de la regle de majorite statistique avec arbitrage par recence.\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Utilisation des cles exactes de votre bloc initial\n",
    "fichier_synthese = DIR_SORTIE / \"SYNTHESE_OSO_FREQUENCE.csv\"\n",
    "CHAMPS_ID = config['CHAMPS_ID']\n",
    "ANNEES_LISTE = sorted([int(a) for a in config['ANNEES_OSO']])\n",
    "\n",
    "# Structure de stockage des donnees chronologiques\n",
    "base_globale = defaultdict(dict)\n",
    "\n",
    "# --- 2. CONSOLIDATION DES DONNEES ANNUELLES ---\n",
    "print(\"INFO : Phase de lecture des fichiers annuels OSO...\")\n",
    "for annee in config['ANNEES_OSO']:\n",
    "    # Utilisation du dossier specifie dans votre config\n",
    "    chemin = config['DOSSIER_CSV_OSO'] / f\"STAT_PARCELLES_OSO_{annee}.csv\"\n",
    "    \n",
    "    if not chemin.exists():\n",
    "        continue\n",
    "        \n",
    "    with open(chemin, mode='r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter=';')\n",
    "        for row in reader:\n",
    "            # Recuperation securisee du champ PL ou creation par concatenation\n",
    "            if 'PL' in row:\n",
    "                id_pl = str(row['PL']).strip()\n",
    "            else:\n",
    "                # Securite si le champ PL n'a pas ete genere dans le module precedent\n",
    "                id_pl = \"\".join([str(row[c]).strip() for c in CHAMPS_ID])\n",
    "                \n",
    "            base_globale[id_pl][int(annee)] = {\n",
    "                'DOM': row['DOMINANTE'],\n",
    "                'PROB': float(row['PROBA_DOMINANTE']) if row['PROBA_DOMINANTE'] else 0.0,\n",
    "                'ID_ATTRS': [row[c] for c in CHAMPS_ID]\n",
    "            }\n",
    "\n",
    "# --- 3. CALCUL DE LA DECISION FINALE ---\n",
    "print(\"INFO : Calcul des frequences et arbitrage par chronologie inverse...\")\n",
    "with open(fichier_synthese, mode='w', newline='', encoding='utf-8') as f_out:\n",
    "    w = csv.writer(f_out, delimiter=';')\n",
    "    \n",
    "    # Construction de l'entete de sortie\n",
    "    entete = [\"PL\"] + CHAMPS_ID\n",
    "    for a in ANNEES_LISTE:\n",
    "        entete += [f\"DOM_{a}\", f\"PROB_{a}\"]\n",
    "    w.writerow(entete + [\"DOM_OSO_FINALE\", \"SCORE_STABILITE\", \"PROBA_MOY_ELU\", \"METHODE\"])\n",
    "\n",
    "    for id_pl, data in base_globale.items():\n",
    "        historique_classes = [data[a]['DOM'] for a in ANNEES_LISTE if a in data]\n",
    "        if not historique_classes:\n",
    "            continue\n",
    "\n",
    "        # Comptage des occurrences par classe\n",
    "        counts = defaultdict(int)\n",
    "        for c in historique_classes:\n",
    "            if c and c != \"N/A\":\n",
    "                counts[c] += 1\n",
    "        \n",
    "        if not counts:\n",
    "            continue\n",
    "\n",
    "        # Determination de la frequence maximale\n",
    "        max_freq = max(counts.values())\n",
    "        gagnants = [cl for cl, nb in counts.items() if nb == max_freq]\n",
    "\n",
    "        # Logique d'arbitrage (Majorite puis Recence)\n",
    "        if len(gagnants) == 1:\n",
    "            elu = gagnants[0]\n",
    "            methode = \"MAJORITE_FREQUENCE\"\n",
    "        else:\n",
    "            elu = None\n",
    "            for a_check in reversed(ANNEES_LISTE):\n",
    "                classe_annee = data.get(a_check, {}).get('DOM')\n",
    "                if classe_annee in gagnants:\n",
    "                    elu = classe_annee\n",
    "                    break\n",
    "            methode = \"ARBITRAGE_DATE_RECENTE\"\n",
    "\n",
    "        # Calcul des indicateurs de fiabilite\n",
    "        score_stabilite = round((counts[elu] / len(ANNEES_LISTE)) * 100, 1)\n",
    "        probas_elu = [data[a]['PROB'] for a in data if data[a].get('DOM') == elu]\n",
    "        proba_moyenne = round(sum(probas_elu) / len(probas_elu), 1) if probas_elu else 0.0\n",
    "\n",
    "        # Preparation de l'export\n",
    "        premiere_date = next(iter(data))\n",
    "        identifiants = data[premiere_date]['ID_ATTRS']\n",
    "        \n",
    "        ligne = [id_pl] + identifiants\n",
    "        for a in ANNEES_LISTE:\n",
    "            ligne += [data.get(a, {}).get('DOM', \"N/A\"), data.get(a, {}).get('PROB', 0.0)]\n",
    "        \n",
    "        ligne += [elu, score_stabilite, proba_moyenne, methode]\n",
    "        w.writerow(ligne)\n",
    "\n",
    "print(f\"INFO : Synthese terminee. Resultats : {fichier_synthese.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74404e73-8bd8-4f51-9043-a1ca6b988d60",
   "metadata": {},
   "source": [
    "# IV. Intégration du référentiel BD FORÊT V2 2017 (source IGN)\n",
    "\n",
    "### 4.1. Description technique\n",
    "Ce module exploite la base de données forestière de l'IGN (BD FORÊT® v2) pour caractériser l'occupation forestière et semi-naturelle.\n",
    "* **Typologie des formations végétales (TFV)** : Le script utilise une fonction de classification (`classifier_tfv`) pour harmoniser la nomenclature IGN. Dans cette version, les formations **\"herbacées\"** ainsi que les **\"landes\"** sont rattachées à la classe **NATURE**, tandis que les formations arborées sont classées en **BOIS**.\n",
    "* **Calcul du Reliquat (\"AUTRE\")** : Le script identifie par soustraction les espaces de la parcelle non couverts par la BD FORÊT, permettant de maintenir une cohérence surfacique totale par rapport au référentiel cadastral.\n",
    "\n",
    "### 4.2. Logique de fiabilité et domaine de compétence\n",
    "La gestion de la probabilité de confiance dans ce module repose sur une approche binaire stricte :\n",
    "* **Confiance Maximale (100)** : Une valeur de 100 est attribuée aux classes **BOIS** et **NATURE**. En l'absence de contre-indication sur ce référentiel institutionnel, on considère la donnée IGN comme une \"vérité terrain\" absolue pour ces thématiques.\n",
    "* **Neutralité de Compétence (0)** : La valeur de probabilité est fixée à **0** pour la classe **AUTRE**. Ce choix méthodologique souligne que la BD FORÊT n'est pas compétente pour qualifier les espaces non forestiers (bâti, agriculture, etc.). Cela permet, lors de la fusion finale, de ne pas donner de poids à une information de \"non-présence\" issue d'une source spécialisée.\n",
    "\n",
    "### 4.3. Justification Méthodologique\n",
    "1. **Précision de l'overlay** : L'utilisation d'une intersection géométrique stricte (`gpd.overlay`) assure un calcul de surface au mètre carré près.\n",
    "2. **Référentiel de référence** : La BD FORÊT sert de base stable pour confirmer la présence historique de végétation pérenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e78fd6-8a3a-4bb1-a33b-eb74e66fe75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Analyse spatiale BD FORET - Millesime 2017\n",
      "SUCCES : Fichier BD FORET genere -> STAT_PARCELLES_FORET_2017.csv\n",
      "TRAITEMENT BD FORET TERMINE.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE : ANALYSE STATISTIQUE BD FORÊT (IGN)\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "print(\"INFO : Analyse spatiale BD FORET - Millesime 2017\")\n",
    "\n",
    "# --- 1. CHARGEMENT ET SECURISATION DES TYPES ---\n",
    "for col in config['CHAMPS_ID']:\n",
    "    if col in gdf_parcelles.columns:\n",
    "        gdf_parcelles[col] = gdf_parcelles[col].astype(str).str.strip()\n",
    "\n",
    "try:\n",
    "    lyr_foret = gpd.read_file(\n",
    "        config['FORET_PATH'], \n",
    "        layer=\"FORMATION_VEGETALE_BD_FORET_2017\", \n",
    "        mask=gdf_parcelles, \n",
    "        engine='pyogrio'\n",
    "    )\n",
    "    if lyr_foret.crs != gdf_parcelles.crs:\n",
    "        lyr_foret = lyr_foret.to_crs(gdf_parcelles.crs)\n",
    "except Exception as e:\n",
    "    print(f\"ERREUR : Impossible de charger la couche Foret : {e}\")\n",
    "    lyr_foret = None\n",
    "\n",
    "if lyr_foret is not None:\n",
    "    # --- 2. TRAITEMENT GEOMETRIQUE ---\n",
    "    inter = gpd.overlay(lyr_foret, gdf_parcelles, how='intersection')\n",
    "    \n",
    "    if not inter.empty:\n",
    "        inter['S_INT'] = inter.geometry.area\n",
    "        \n",
    "        def classifier_tfv(tfv):\n",
    "            tfv_str = str(tfv).lower()\n",
    "            if \"lande\" in tfv_str: return \"NATURE\"\n",
    "            elif \"herbacée\" in tfv_str: return \"NATURE\"\n",
    "            else: return \"BOIS\"\n",
    "\n",
    "        inter['CLASSE_NOM'] = inter['TFV'].apply(classifier_tfv)\n",
    "\n",
    "        # Agregation par parcelle\n",
    "        stats = inter.groupby(config['CHAMPS_ID'] + ['CLASSE_NOM'])['S_INT'].sum().reset_index()\n",
    "        df_res = stats.pivot_table(index=config['CHAMPS_ID'], columns='CLASSE_NOM', values='S_INT').fillna(0)\n",
    "\n",
    "        # --- 3. CALCUL DES INDICATEURS ---\n",
    "        classes_foret = [\"BOIS\", \"NATURE\"]\n",
    "        for cl in classes_foret:\n",
    "            if cl not in df_res.columns: df_res[cl] = 0.0\n",
    "\n",
    "        df_res = df_res.reset_index()\n",
    "        surf_tot = gdf_parcelles[config['CHAMPS_ID'] + ['geometry']].copy()\n",
    "        \n",
    "        # Fusion pour recuperer la géométrie de reference\n",
    "        df_res = pd.merge(df_res, surf_tot, on=config['CHAMPS_ID'], how='right').fillna(0)\n",
    "        \n",
    "        # SECURITE : On force la conversion en GeoDataFrame pour pouvoir utiliser .area\n",
    "        df_res = gpd.GeoDataFrame(df_res, geometry='geometry', crs=gdf_parcelles.crs)\n",
    "        \n",
    "        # Calcul de la surface totale arrondie\n",
    "        df_res['SURF_TOTALE_M2'] = df_res.geometry.area.round(1)\n",
    "\n",
    "        # Calcul de la classe AUTRE (Reste de la parcelle)\n",
    "        df_res['AUTRE'] = (df_res['SURF_TOTALE_M2'] - df_res[classes_foret].sum(axis=1)).clip(lower=0)\n",
    "        \n",
    "        # Calcul des pourcentages (arrondis a 2 decimales)\n",
    "        classes_finales = [\"BOIS\", \"NATURE\", \"AUTRE\"]\n",
    "        for cl in classes_finales:\n",
    "            df_res[f\"{cl}_%\"] = ((df_res[cl] / df_res['SURF_TOTALE_M2']) * 100).round(2)\n",
    "\n",
    "        # Determination de la dominante\n",
    "        df_res['DOMINANTE'] = df_res[[f\"{cl}_%\" for cl in classes_finales]].idxmax(axis=1).str.replace('_%', '')\n",
    "\n",
    "        # --- 4. COLONNE PROBA (PROD) ---\n",
    "        df_res['PROBA_DOMINANTE'] = df_res['DOMINANTE'].apply(lambda x: 100 if x != \"AUTRE\" else 0)\n",
    "\n",
    "        # --- 5. GENERATION DU CHAMP 'PL' (TEXTE PUR) ---\n",
    "        df_res['PL'] = (\n",
    "            df_res['CODE_DEP'].astype(str) + \n",
    "            df_res['CODE_COM'].astype(str) + \n",
    "            df_res['SECTION'].astype(str) + \n",
    "            df_res['NUMERO'].astype(str)\n",
    "        )\n",
    "\n",
    "        # --- 6. EXPORTATION ---\n",
    "        output_csv = config['DOSSIER_CSV_FORET'] / \"STAT_PARCELLES_FORET_2017.csv\"\n",
    "        cols_finales = ['PL'] + config['CHAMPS_ID'] + [f\"{cl}_%\" for cl in classes_finales] + \\\n",
    "                       [\"DOMINANTE\", \"PROBA_DOMINANTE\", \"SURF_TOTALE_M2\"]\n",
    "        \n",
    "        df_res[cols_finales].to_csv(output_csv, sep=';', index=False, encoding='utf-8')\n",
    "        print(f\"SUCCES : Fichier BD FORET genere -> {output_csv.name}\")\n",
    "\n",
    "print(\"TRAITEMENT BD FORET TERMINE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9891a5-8653-4921-8a9d-c51b7292d548",
   "metadata": {},
   "source": [
    "# V. Caractérisation par le référentiel BD TOPO (source IGN)\n",
    "\n",
    "### 5.1. Description technique : Approche multi-couches\n",
    "Ce module exploite la précision métrique de la BD TOPO® pour affiner le diagnostic territorial. Contrairement aux modules précédents, il traite un ensemble hétérogène de thématiques :\n",
    "* **Objets structuraux** : Intersection systématique avec les couches de bâti, d'équipements (cimetières, sport) et d'hydrographie. Chaque couche est redirigée vers une classe cible (ex: Bâtiment → **ARTIFICIALISE**).\n",
    "* **Cohérence spatiale** : Utilisation d'un `overlay` par parcelle pour calculer le taux d'emprise précis de chaque infrastructure ou milieu naturel.\n",
    "\n",
    "### 5.2. Logique de fiabilité et domaine de compétence\n",
    "La gestion de la confiance suit la même rigueur méthodologique que le module forestier :\n",
    "* **Confiance institutionnelle (100)** : S'agissant du référentiel topographique de référence en France, une probabilité de 100 est assignée à toutes les classes identifiées par l'IGN.\n",
    "* **Neutralité du reliquat (0)** : La classe **AUTRE** (surface non couverte par une couche BD TOPO) reçoit une probabilité de **0**. Cela signifie que la BD TOPO ne se prononce pas sur l'occupation de ces espaces, laissant le champ libre aux autres sources (ex : OSO/RPG) lors de la fusion finale.\n",
    "\n",
    "### 5.3. Justification méthodologique\n",
    "1. **Précision du bâti** : La BD TOPO est la seule source capable de garantir la détection des petites structures artificialisées (réservoirs, constructions secondaires) souvent invisibles par satellite.\n",
    "2. **Identification des zones humides et sportives** : Elle apporte des précisions sémantiques cruciales sur les plans d'eau et les terrains de sport, souvent confondus avec d'autres classes dans les classifications automatiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82c95bad-a53b-43d8-98b3-66857c1ef909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> DÉMARRAGE DE L'ANALYSE BD TOPO (SANS VÉGÉTATION)\n",
      "    > Intersection spatiale : BATIMENT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liege\\anaconda3\\Lib\\site-packages\\geopandas\\io\\file.py:576: UserWarning: Error parsing datetimes, original strings are returned: Out of bounds nanosecond timestamp: 1500-01-01, at position 92. You might want to try:\n",
      "    - passing `format` if your strings have a consistent format;\n",
      "    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n",
      "    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.\n",
      "  return pyogrio.read_dataframe(path_or_bytes, bbox=bbox, **kwargs)\n",
      "C:\\Users\\liege\\anaconda3\\Lib\\site-packages\\geopandas\\tools\\overlay.py:358: UserWarning: `keep_geom_type=True` in overlay resulted in 7 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  result = _collection_extract(result, geom_type, keep_geom_type_warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    > Intersection spatiale : CIMETIERE\n",
      "    > Intersection spatiale : CONSTRUCTION_SURFACIQUE\n",
      "    > Intersection spatiale : TERRAIN_DE_SPORT\n",
      "    > Intersection spatiale : SURFACE_HYDROGRAPHIQUE\n",
      "    > Intersection spatiale : RESERVOIR\n",
      "    > Intersection spatiale : ZONE_D_ESTRAN\n",
      "    > Consolidation des indicateurs statistiques...\n",
      ">>> ANALYSE TERMINÉE. FICHIER GÉNÉRÉ : STAT_PARCELLES_BDTOPO_2025.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE : ANALYSE STATISTIQUE BD TOPO (IGN)\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\">>> DÉMARRAGE DE L'ANALYSE BD TOPO (SANS VÉGÉTATION)\")\n",
    "\n",
    "# 1. PARAMÉTRAGE ET INITIALISATION\n",
    "C_ID = config['CHAMPS_ID']  \n",
    "TOPO_PATH = config['BDTOPO_PATH']\n",
    "OUT_DIR = config['DOSSIER_CSV_BDTOPO']\n",
    "\n",
    "if 'gdf_parcelles' not in locals():\n",
    "    raise EnvironmentError(\"Référentiel 'gdf_parcelles' absent. Exécuter le Module 0.\")\n",
    "\n",
    "# Définition des classes cibles\n",
    "classes_list = [\"ARTIFICIALISE\", \"AGRICULTURE\", \"NATURE\", \"BOIS\", \"EAU\", \"AUTRE\"]\n",
    "\n",
    "# Initialisation du GeoDataFrame de calcul\n",
    "df_travail = gdf_parcelles[C_ID + ['geometry']].copy()\n",
    "for cl in classes_list:\n",
    "    df_travail[cl] = 0.0\n",
    "\n",
    "# 2. DÉFINITION DU MAPPING (Couches structurelles uniquement)\n",
    "mapping_layers = {\n",
    "    'BATIMENT': 'ARTIFICIALISE', \n",
    "    'CIMETIERE': 'ARTIFICIALISE', \n",
    "    'CONSTRUCTION_SURFACIQUE': 'ARTIFICIALISE', \n",
    "    'TERRAIN_DE_SPORT': 'ARTIFICIALISE',\n",
    "    'SURFACE_HYDROGRAPHIQUE': 'EAU', \n",
    "    'RESERVOIR': 'ARTIFICIALISE',\n",
    "    'ZONE_D_ESTRAN': 'NATURE'\n",
    "}\n",
    "\n",
    "# 3. TRAITEMENT DES COUCHES (INTERSECTIONS)\n",
    "for layer_name, cat in mapping_layers.items():\n",
    "    try:\n",
    "        print(f\"    > Intersection spatiale : {layer_name}\")\n",
    "        lyr_topo = gpd.read_file(TOPO_PATH, layer=layer_name, mask=gdf_parcelles, engine='pyogrio')\n",
    "        \n",
    "        if not lyr_topo.empty:\n",
    "            if lyr_topo.crs != df_travail.crs:\n",
    "                lyr_topo = lyr_topo.to_crs(df_travail.crs)\n",
    "            \n",
    "            inter = gpd.overlay(df_travail, lyr_topo, how='intersection')\n",
    "            if not inter.empty:\n",
    "                inter['S_INT'] = inter.geometry.area\n",
    "                sums = inter.groupby(C_ID)['S_INT'].sum()\n",
    "                \n",
    "                # Agrégation\n",
    "                df_travail.set_index(C_ID, inplace=True)\n",
    "                df_travail[cat] = df_travail[cat].add(sums, fill_value=0)\n",
    "                df_travail.reset_index(inplace=True)\n",
    "    except Exception as e:\n",
    "        print(f\"      ! Information : Couche {layer_name} non traitée : {e}\")\n",
    "\n",
    "# 4. CONSOLIDATION ET GÉNÉRATION DES INDICATEURS\n",
    "print(\"    > Consolidation des indicateurs statistiques...\")\n",
    "\n",
    "# Calcul surface totale\n",
    "df_travail['SURF_TOTALE_M2'] = df_travail.geometry.area\n",
    "\n",
    "# Calcul de 'AUTRE' (Tout ce qui n'est pas Bâtiment, Eau ou Estran tombera ici)\n",
    "surfaces_identifiees = df_travail[[\"ARTIFICIALISE\", \"AGRICULTURE\", \"NATURE\", \"BOIS\", \"EAU\"]].sum(axis=1)\n",
    "df_travail['AUTRE'] = (df_travail['SURF_TOTALE_M2'] - surfaces_identifiees).clip(lower=0)\n",
    "\n",
    "# Calcul des pourcentages\n",
    "for cl in classes_list:\n",
    "    df_travail[f\"{cl}_%\"] = ((df_travail[cl] / df_travail['SURF_TOTALE_M2']) * 100).round(2)\n",
    "\n",
    "# Détermination dominante et probabilité\n",
    "df_travail['DOMINANTE'] = df_travail[classes_list].idxmax(axis=1)\n",
    "df_travail['PROBA_DOMINANTE'] = np.where(df_travail['DOMINANTE'] == 'AUTRE', 0, 100)\n",
    "\n",
    "# Identifiant PL\n",
    "df_travail['PL'] = (\n",
    "    df_travail['CODE_DEP'].astype(str).str.zfill(2) + \n",
    "    df_travail['CODE_COM'].astype(str).str.zfill(3) + \n",
    "    df_travail['SECTION'].astype(str).str.zfill(2) + \n",
    "    df_travail['NUMERO'].astype(str).str.zfill(4)\n",
    ")\n",
    "\n",
    "# Export\n",
    "output_path = Path(OUT_DIR) / \"STAT_PARCELLES_BDTOPO_2025.csv\"\n",
    "cols_export = ['PL'] + C_ID + [f\"{cl}_%\" for cl in classes_list] + [\"DOMINANTE\", \"PROBA_DOMINANTE\", \"SURF_TOTALE_M2\"]\n",
    "df_travail[cols_export].to_csv(output_path, sep=';', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\">>> ANALYSE TERMINÉE. FICHIER GÉNÉRÉ : {output_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2375d737-4e58-48ec-9c8d-0e4b12700fed",
   "metadata": {},
   "source": [
    "# VI. Consolidation et synthèse temporelle du registre parcellaire (RPG)\n",
    "\n",
    "### 6.1. Module 1 : Fusion multi-sources (Standard + Complet)\n",
    "Ce module traite le RPG comme un assemblage de sources distinctes et complémentaires. À l'instar de la BD TOPO, le script interroge deux flux indépendants pour caractériser une même unité foncière :\n",
    "* **Indépendance des flux** : Le **RPG Standard** et le **RPG Complet** sont chargés et intersectés successivement. Le Standard assure la structure administrative globale, tandis que le Complet apporte des précisions sur ce qui n'apparaît pas dans le premier flux.\n",
    "* **Agrégation spatiale** : Les surfaces issues de chaque source sont cumulées par parcelle. L'utilisation de l'identifiant `PL` garantit que chaque apport est correctement localisé sans créer de doublons géométriques, chaque source venant enrichir le bilan surfacique de la parcelle.\n",
    "* **Fiabilité différenciée** : \n",
    "   $$PROBA\\_PONDEREE = \\frac{\\sum (Surface \\cap \\times Confiance)}{\\sum Surface \\cap}$$\n",
    "   Une confiance de 100 est assignée au Standard. Pour le Complet, la probabilité native est conservée. Le reliquat non couvert par ces deux sources est classé en **\"AUTRE\"** (confiance 0).\n",
    "\n",
    "\n",
    "\n",
    "### 6.2. Module 2 : Arbitrage temporel et stabilité (2018-2024)\n",
    "L'objectif est de définir l'usage agricole stable sur le long terme à partir des données consolidées annuellement.\n",
    "* **Logique d'arbitrage** : \n",
    "    1. **Fréquence** : La classe la plus présente sur la période est élue (majorité).\n",
    "    2. **Récence** : En cas d'égalité, le script utilise la chronologie inverse (depuis 2024) pour trancher.\n",
    "* **Indicateurs** : Calcul du **Score de stabilité** et de la **Proba moyenne** pour valider la robustesse du diagnostic.\n",
    "\n",
    "### 6.3. Justification méthodologique\n",
    "Cette approche traite le RPG comme un système multi-sources cohérent. La distinction entre Standard et Complet permet d'exploiter chaque gisement de données sans perte d'information. La neutralité de la classe \"AUTRE\" assure qu'en l'absence de déclaration agricole, le système laisse la priorité aux autres référentiels lors de la fusion finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89cc7a86-7b09-4e84-a289-8d6770902fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DÉMARRAGE DE LA FUSION HARMONISÉE RPG ---\n",
      "\n",
      ">>> Traitement de l'annuité : 2018\n",
      "   -> Fichier généré : STAT_RPG_SYNTHESE_2018.csv\n",
      "\n",
      ">>> Traitement de l'annuité : 2019\n",
      "   -> Fichier généré : STAT_RPG_SYNTHESE_2019.csv\n",
      "\n",
      ">>> Traitement de l'annuité : 2020\n",
      "   -> Fichier généré : STAT_RPG_SYNTHESE_2020.csv\n",
      "\n",
      ">>> Traitement de l'annuité : 2021\n",
      "   -> Fichier généré : STAT_RPG_SYNTHESE_2021.csv\n",
      "\n",
      ">>> Traitement de l'annuité : 2022\n",
      "   -> Fichier généré : STAT_RPG_SYNTHESE_2022.csv\n",
      "\n",
      ">>> Traitement de l'annuité : 2023\n",
      "   ! Note : La couche RPG_COMPLETE_2023 n'a pas pu être traitée (Ignoré).\n",
      "   -> Fichier généré : STAT_RPG_SYNTHESE_2023.csv\n",
      "\n",
      ">>> Traitement de l'annuité : 2024\n",
      "   ! Note : La couche RPG_COMPLETE_2024 n'a pas pu être traitée (Ignoré).\n",
      "   -> Fichier généré : STAT_RPG_SYNTHESE_2024.csv\n",
      "\n",
      "--- ANALYSE TERMINÉE : SÉCURITÉ ACTIVE & RÈGLES APPLIQUÉES ---\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE : FUSION HARMONISÉE RPG\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"--- DÉMARRAGE DE LA FUSION HARMONISÉE RPG ---\")\n",
    "\n",
    "# 1. PARAMÉTRAGE ET RÉFÉRENTIELS\n",
    "C_ID = config['CHAMPS_ID']\n",
    "ANNEES_RPG = config['ANNEES_RPG']\n",
    "PATH_RPG_STD = config['RPG_GPKG']\n",
    "PATH_RPG_COMP = config['RPG_COMP_GPKG']\n",
    "DIR_FUSION = config['DIR_CSV_RPG_FUS']\n",
    "\n",
    "# Nomenclature (On garde BOIS dans la liste pour éviter les erreurs si appelé ailleurs)\n",
    "CLASSES_CIBLES = [\"ARTIFICIALISE\", \"AGRICULTURE\", \"NATURE\", \"BOIS\", \"EAU\", \"AUTRE\"]\n",
    "\n",
    "def reclass_unifie(row, est_std):\n",
    "    \"\"\"\n",
    "    Standardisation selon tes règles spécifiques avec protection technique.\n",
    "    \"\"\"\n",
    "    if est_std:\n",
    "        # --- CAS 1 : RPG STANDARD (CODE_GROUP) ---\n",
    "        try:\n",
    "            val = row.get(\"CODE_GROUP\", 0)\n",
    "            grp = int(val) if val is not None else 0\n",
    "        except (ValueError, TypeError):\n",
    "            grp = 0\n",
    "            \n",
    "        if grp == 17: return \"NATURE\"\n",
    "        return \"AGRICULTURE\"\n",
    "        \n",
    "    else:\n",
    "        # --- CAS 2 : RPG COMPLET (culture) ---\n",
    "        label = str(row.get(\"culture\", \"\"))\n",
    "        if not label or label == \"None\":\n",
    "            return \"AUTRE\"\n",
    "        \n",
    "        l_low = label.lower()\n",
    "        \n",
    "        # --- RÈGLES STRICTES ---\n",
    "        if \"jardin collectif\" in l_low: return \"ARTIFICIALISE\"\n",
    "        if \"landes\" in l_low: return \"NATURE\"\n",
    "        if \"landes (pelouses)\" in l_low: return \"NATURE\"    \n",
    "        \n",
    "        return \"AGRICULTURE\"\n",
    "\n",
    "# --- EXÉCUTION DU FLUX TEMPOREL ---\n",
    "for annee in ANNEES_RPG:\n",
    "    print(f\"\\n>>> Traitement de l'annuité : {annee}\")\n",
    "    \n",
    "    # PROTECTION : Nettoyage des identifiants (espaces, types)\n",
    "    df_bilan = gdf_parcelles[C_ID + ['geometry']].copy()\n",
    "    for col in C_ID:\n",
    "        df_bilan[col] = df_bilan[col].astype(str).str.strip()\n",
    "        \n",
    "    df_bilan['SURF_CAD'] = df_bilan.geometry.area\n",
    "    \n",
    "    for cl in CLASSES_CIBLES:\n",
    "        df_bilan[f\"{cl}_s\"] = 0.0\n",
    "        df_bilan[f\"{cl}_p\"] = 0.0\n",
    "\n",
    "    def traiter_source(path, layer_name, est_std):\n",
    "        global df_bilan\n",
    "        try:\n",
    "            lyr_src = gpd.read_file(path, layer=layer_name, mask=gdf_parcelles, engine='pyogrio')\n",
    "            \n",
    "            if not lyr_src.empty:\n",
    "                if lyr_src.crs != df_bilan.crs:\n",
    "                    lyr_src = lyr_src.to_crs(df_bilan.crs)\n",
    "                \n",
    "                lyr_src['geometry'] = lyr_src.geometry.make_valid()\n",
    "                inter = gpd.overlay(lyr_src, df_bilan[C_ID + ['geometry']], how='intersection')\n",
    "                \n",
    "                if not inter.empty:\n",
    "                    # PROTECTION : Nettoyage post-intersection\n",
    "                    for col in C_ID:\n",
    "                        inter[col] = inter[col].astype(str).str.strip()\n",
    "                        \n",
    "                    inter['S_INT'] = inter.geometry.area\n",
    "                    inter['CLASSE_UNIF'] = inter.apply(lambda r: reclass_unifie(r, est_std), axis=1)\n",
    "                    \n",
    "                    # PROTECTION : Correction technique des probabilités (np.where)\n",
    "                    if not est_std and 'proba' in inter.columns:\n",
    "                        p_raw = inter['proba'].astype(str).str.replace(',', '.').astype(float).fillna(0.0)\n",
    "                        inter['p_val'] = np.where(p_raw <= 1.0, p_raw * 100.0, p_raw)\n",
    "                    else:\n",
    "                        inter['p_val'] = 100.0\n",
    "                    \n",
    "                    inter['P_POND'] = inter['S_INT'] * inter['p_val']\n",
    "                    \n",
    "                    for cl in CLASSES_CIBLES:\n",
    "                        mask_cl = inter['CLASSE_UNIF'] == cl\n",
    "                        if mask_cl.any():\n",
    "                            subset = inter[mask_cl].groupby(C_ID).agg({'S_INT': 'sum', 'P_POND': 'sum'})\n",
    "                            df_bilan = df_bilan.set_index(C_ID)\n",
    "                            df_bilan[f\"{cl}_s\"] = df_bilan[f\"{cl}_s\"].add(subset['S_INT'], fill_value=0)\n",
    "                            df_bilan[f\"{cl}_p\"] = df_bilan[f\"{cl}_p\"].add(subset['P_POND'], fill_value=0)\n",
    "                            df_bilan = df_bilan.reset_index()\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"   ! Note : La couche {layer_name} n'a pas pu être traitée (Ignoré).\")\n",
    "\n",
    "    traiter_source(PATH_RPG_STD, f\"RPG_{annee}\", est_std=True)\n",
    "    traiter_source(PATH_RPG_COMP, f\"RPG_COMPLETE_{annee}\", est_std=False)\n",
    "\n",
    "    # Finalisation\n",
    "    surf_rpg_tot = df_bilan[[f\"{cl}_s\" for cl in CLASSES_CIBLES]].sum(axis=1)\n",
    "    df_bilan['AUTRE_s'] = (df_bilan['SURF_CAD'] - surf_rpg_tot).clip(lower=0)\n",
    "    \n",
    "    for cl in CLASSES_CIBLES:\n",
    "        df_bilan[f\"{cl}_%\"] = ((df_bilan[f\"{cl}_s\"] / df_bilan['SURF_CAD']) * 100).round(2)\n",
    "    \n",
    "    # PROTECTION : Dominante sécurisée\n",
    "    df_bilan['DOMINANTE'] = np.where(\n",
    "        surf_rpg_tot > 0,\n",
    "        df_bilan[[f\"{cl}_s\" for cl in CLASSES_CIBLES]].idxmax(axis=1).str.replace('_s', ''),\n",
    "        \"AUTRE\"\n",
    "    )\n",
    "    \n",
    "    def calculer_proba(row):\n",
    "        dom = row['DOMINANTE']\n",
    "        if dom == \"AUTRE\": return 0.0\n",
    "        return round(row[f\"{dom}_p\"] / row[f\"{dom}_s\"], 1) if row[f\"{dom}_s\"] > 0 else 0.0\n",
    "\n",
    "    df_bilan['PROBA_PONDEREE'] = df_bilan.apply(calculer_proba, axis=1)\n",
    "\n",
    "    # Identifiant PL propre\n",
    "    df_bilan['PL'] = (\n",
    "        df_bilan['CODE_DEP'].astype(str).str.zfill(2) + \n",
    "        df_bilan['CODE_COM'].astype(str).str.zfill(3) + \n",
    "        df_bilan['SECTION'].astype(str).str.zfill(2) + \n",
    "        df_bilan['NUMERO'].astype(str).str.zfill(4)\n",
    "    )\n",
    "\n",
    "    path_out = Path(DIR_FUSION) / f\"STAT_RPG_SYNTHESE_{annee}.csv\"\n",
    "    cols = ['PL'] + C_ID + [f\"{cl}_%\" for cl in CLASSES_CIBLES] + [\"DOMINANTE\", \"PROBA_PONDEREE\", \"SURF_CAD\"]\n",
    "    \n",
    "    path_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df_bilan[cols].to_csv(path_out, sep=';', index=False, encoding='utf-8')\n",
    "    print(f\"   -> Fichier généré : {path_out.name}\")\n",
    "\n",
    "print(\"\\n--- ANALYSE TERMINÉE : SÉCURITÉ ACTIVE & RÈGLES APPLIQUÉES ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae9ea198-e318-4c8f-8d1c-f97436157203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> DÉMARRAGE DE LA SYNTHÈSE TEMPORELLE RPG\n",
      "    > Lecture des historiques (2018 - 2024)...\n",
      "    > Calcul de l'arbitrage (Majorité vs Récence)...\n",
      ">>> SYNTHÈSE TERMINÉE : SYNTHESE_RPG_FREQUENCE.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE : SYNTHÈSE TEMPORELLE RPG (ARBITRAGE ET STABILITÉ)\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from pathlib import Path\n",
    "\n",
    "print(\">>> DÉMARRAGE DE LA SYNTHÈSE TEMPORELLE RPG\")\n",
    "\n",
    "# --- 1. SÉCURISATION DE LA CONFIGURATION ---\n",
    "DIR_FUSION = config.get('DIR_CSV_RPG_FUS')\n",
    "DIR_SORTIE = config.get('DIR_SORTIE', globals().get('DIR_SORTIE'))\n",
    "CHAMPS_ID  = config.get('CHAMPS_ID')\n",
    "ANNEES_RPG = sorted([int(a) for a in config.get('ANNEES_RPG', [])])\n",
    "\n",
    "if not DIR_SORTIE:\n",
    "    raise ValueError(\"Erreur : DIR_SORTIE n'est pas défini dans la config.\")\n",
    "\n",
    "fichier_synthese = Path(DIR_SORTIE) / \"SYNTHESE_RPG_FREQUENCE.csv\"\n",
    "base_globale = defaultdict(dict)\n",
    "\n",
    "# --- 2. LECTURE DES DONNÉES ANNUELLES ---\n",
    "print(f\"    > Lecture des historiques ({min(ANNEES_RPG)} - {max(ANNEES_RPG)})...\")\n",
    "for annee in ANNEES_RPG:\n",
    "    chemin = Path(DIR_FUSION) / f\"STAT_RPG_SYNTHESE_{annee}.csv\"\n",
    "    if not chemin.exists(): continue\n",
    "    \n",
    "    # Lecture forcée en string pour préserver les zéros (01, 005...)\n",
    "    df_annuel = pd.read_csv(chemin, sep=';', encoding='utf-8', dtype={c: str for c in CHAMPS_ID})\n",
    "    \n",
    "    for _, row in df_annuel.iterrows():\n",
    "        # --- RÉPARATION DES IDENTIFIANTS (Sécurité Dalles 1 & 4) ---\n",
    "        # On crée un dictionnaire temporaire pour formater chaque champ proprement\n",
    "        d = {c: str(row[c]).strip() for c in CHAMPS_ID}\n",
    "        \n",
    "        # Application du zfill sur les codes numériques pour l'alignement Cadastre\n",
    "        if 'CODE_DEP' in d: d['CODE_DEP'] = d['CODE_DEP'].zfill(2)\n",
    "        if 'CODE_COM' in d: d['CODE_COM'] = d['CODE_COM'].zfill(3)\n",
    "        if 'SECTION' in d:  d['SECTION']  = d['SECTION'].zfill(2)\n",
    "        if 'NUMERO' in d:   d['NUMERO']   = d['NUMERO'].zfill(4)\n",
    "        \n",
    "        # La clé est un tuple des valeurs dans l'ordre de CHAMPS_ID\n",
    "        cle = tuple(d[c] for c in CHAMPS_ID)\n",
    "        \n",
    "        try:\n",
    "            proba = float(str(row['PROBA_PONDEREE']).replace(',', '.'))\n",
    "        except:\n",
    "            proba = 0.0\n",
    "            \n",
    "        base_globale[cle][annee] = {\n",
    "            'DOM': row.get('DOMINANTE', 'AUTRE'),\n",
    "            'PROB': proba\n",
    "        }\n",
    "\n",
    "# --- 3. ARBITRAGE ET CALCULS ---\n",
    "print(\"    > Calcul de l'arbitrage (Majorité vs Récence)...\")\n",
    "final_data = []\n",
    "\n",
    "for cle, data in base_globale.items():\n",
    "    historique = {a: data[a]['DOM'] for a in ANNEES_RPG if a in data}\n",
    "    if not historique: continue\n",
    "\n",
    "    classes_obs = list(historique.values())\n",
    "    counts = Counter(classes_obs)\n",
    "    \n",
    "    max_freq = max(counts.values())\n",
    "    gagnants = [cl for cl, nb in counts.items() if nb == max_freq]\n",
    "\n",
    "    if len(gagnants) == 1:\n",
    "        elu = gagnants[0]\n",
    "        methode = \"MAJORITE\"\n",
    "    else:\n",
    "        elu = next((historique[a] for a in reversed(ANNEES_RPG) if a in historique and historique[a] in gagnants), None)\n",
    "        methode = \"RECENCE_EGALITE\"\n",
    "\n",
    "    score_stabilite = round((counts[elu] / len(ANNEES_RPG)) * 100, 1)\n",
    "    probas_elu = [data[a]['PROB'] for a in data if a in historique and historique[a] == elu]\n",
    "    proba_moyenne = round(sum(probas_elu) / len(probas_elu), 1) if probas_elu else 0.0\n",
    "\n",
    "    # --- RECONSTRUCTION DU PL (Basée sur tes colonnes précises) ---\n",
    "    # On crée un mapping pour retrouver facilement les valeurs par nom de colonne\n",
    "    m = dict(zip(CHAMPS_ID, cle))\n",
    "    \n",
    "    # PL = DEP(2) + COM(3) + SECTION(2) + NUMERO(4)\n",
    "    # On utilise .get() pour éviter tout plantage si une colonne manquait\n",
    "    id_pl = m.get('CODE_DEP', '') + m.get('CODE_COM', '') + m.get('SECTION', '') + m.get('NUMERO', '')\n",
    "    \n",
    "    ligne = {'PL': id_pl}\n",
    "    # Remplissage dynamique pour éviter l'IndexError\n",
    "    for i, champ in enumerate(CHAMPS_ID):\n",
    "        ligne[champ] = cle[i]\n",
    "        \n",
    "    for a in ANNEES_RPG:\n",
    "        ligne[f\"DOM_{a}\"] = historique.get(a, \"N/A\")\n",
    "        ligne[f\"PROB_{a}\"] = round(data[a]['PROB'], 1) if a in data else 0.0\n",
    "            \n",
    "    ligne.update({\n",
    "        'DOMINANTE': elu,\n",
    "        'SCORE_STABILITE': score_stabilite,\n",
    "        'PROBA_PONDEREE': proba_moyenne,\n",
    "        'LOGIQUE_ARBITRAGE': methode\n",
    "    })\n",
    "    \n",
    "    final_data.append(ligne)\n",
    "\n",
    "# --- 4. EXPORT ---\n",
    "df_final = pd.DataFrame(final_data)\n",
    "df_final.to_csv(fichier_synthese, sep=';', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\">>> SYNTHÈSE TERMINÉE : {fichier_synthese.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb466c4b-0856-486c-8d3b-11275da157cb",
   "metadata": {},
   "source": [
    "# VII. Analyse spatiale et synthèse temporelle COSIA\n",
    "\n",
    "### 7.1. Module 1 : Analyse statistique annuelle (Structure miroir)\n",
    "Ce module traite les données COSIA selon une architecture calquée sur celle de l'OSO, permettant une comparaison directe des résultats. Le processus se distingue par :\n",
    "* **Mapping sémantique rigoureux** : Conversion des codes sources COSIA vers la nomenclature cible (6 classes) avec un nettoyage systématique des caractères (suppression des points) pour garantir la correspondance.\n",
    "* **Intégration de probabilités expertes** : Contrairement à l'OSO , le module COSIA utilise des **probabilités fixes** définies dans la nomenclature. Chaque usage du sol reçoit un indice de fiabilité théorique (`PROBA_VAL`) reflétant la précision historique de la donnée source.\n",
    "* **Calcul de fiabilité pondérée** : \n",
    "  $$PROBA\\_DOMINANTE = \\frac{\\sum (Surface \\cap \\times Confiance\\_fixe)}{\\sum Surface \\cap}$$\n",
    "\n",
    "### 7.2. Module 2 : Synthèse temporelle et arbitrage de cohérence\n",
    "L'objectif est de dégager une tendance stable sur l'ensemble des millésimes COSIA disponibles pour consolider le diagnostic foncier.\n",
    "* **Logique de Décision (Majorité vs Récence)** : \n",
    "    1. **Fréquence** : Élection de la classe la plus représentée sur l'historique pour lisser les variations mineures.\n",
    "    2. **Récence si égalité** : En cas d'ex-æquo (ex: 2 ans Nature / 2 ans Agriculture), le script applique une chronologie inverse en interrogeant le millésime le plus récent pour refléter l'état actuel.\n",
    "* **Indicateurs de Performance** :\n",
    "    * **SCORE_STABILITE** : Mesure du taux de permanence de la classe élue au fil des années.\n",
    "    * **PROBA_MOYENNE** : Synthèse des indices de fiabilité de la nomenclature pour la classe retenue.\n",
    "\n",
    "### 7.3. Justification méthodologique\n",
    "L'analyse COSIA agit comme un **référentiel de validation**. En reproduisant la structure de l'OSO, elle permet de croiser deux visions de l'occupation du sol. La standardisation de l'identifiant `PL` avec formatage `zfill` assure que ces données sont prêtes pour l'arbitrage final avec le RPG et la BD TOPO etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a82c8a7-22dd-41e5-8886-6eaf03865d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO : Analyse spatiale COSIA - Millesime 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liege\\anaconda3\\Lib\\site-packages\\geopandas\\tools\\overlay.py:358: UserWarning: `keep_geom_type=True` in overlay resulted in 45 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  result = _collection_extract(result, geom_type, keep_geom_type_warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCES : Export COSIA millesime 2018 genere.\n",
      "INFO : Analyse spatiale COSIA - Millesime 2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liege\\anaconda3\\Lib\\site-packages\\geopandas\\tools\\overlay.py:358: UserWarning: `keep_geom_type=True` in overlay resulted in 3 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  result = _collection_extract(result, geom_type, keep_geom_type_warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCES : Export COSIA millesime 2021 genere.\n",
      "INFO : Analyse spatiale COSIA - Millesime 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liege\\anaconda3\\Lib\\site-packages\\geopandas\\tools\\overlay.py:358: UserWarning: `keep_geom_type=True` in overlay resulted in 6 dropped geometries of different geometry types than df1 has. Set `keep_geom_type=False` to retain all geometries\n",
      "  result = _collection_extract(result, geom_type, keep_geom_type_warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCES : Export COSIA millesime 2024 genere.\n",
      "TRAITEMENT COSIA TERMINE.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE : ANALYSE STATISTIQUE COSIA (STRUCTURE MIROIR OSO)\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# --- 1. PREPARATION DES REFERENTIELS ET NOMENCLATURES ---\n",
    "classes_list = [\"ARTIFICIALISE\", \"AGRICULTURE\", \"NATURE\", \"BOIS\", \"EAU\", \"AUTRE\"]\n",
    "\n",
    "# Chargement nomenclature COSIA\n",
    "df_nomen = pd.read_csv(config['NOMEN_COSIA_CSV'], sep=';', encoding='utf-8-sig')\n",
    "\n",
    "# Mapping des Classes et des Probabilités fixes issues de la nomenclature\n",
    "# On nettoie le code source (suppression des points) pour la correspondance\n",
    "mapping_cosia = {str(row['CODE_SOURCE']).replace('.', '').strip(): str(row['NATURE_SOL']).strip() \n",
    "                 for _, row in df_nomen.iterrows()}\n",
    "\n",
    "mapping_proba = {str(row['CODE_SOURCE']).replace('.', '').strip(): float(str(row['PROBA_SOURCE']).replace(',', '.')) \n",
    "                 for _, row in df_nomen.iterrows()}\n",
    "\n",
    "# --- 2. TRAITEMENT ANALYTIQUE ANNUEL ---\n",
    "for annee in config['ANNEES_COSIA']:\n",
    "    print(f\"INFO : Analyse spatiale COSIA - Millesime {annee}\")\n",
    "    layer_name = f\"COSIA_VECTEUR_{annee}\"\n",
    "    \n",
    "    try:\n",
    "        # Lecture avec mask pour le minimum d'optimisation vital\n",
    "        lyr_cosia = gpd.read_file(config['COSIA_GPKG'], layer=layer_name, mask=gdf_parcelles, engine='pyogrio')\n",
    "        if lyr_cosia.crs != gdf_parcelles.crs:\n",
    "            lyr_cosia = lyr_cosia.to_crs(gdf_parcelles.crs)\n",
    "    except:\n",
    "        print(f\"ERREUR : Couche {layer_name} introuvable.\")\n",
    "        continue\n",
    "\n",
    "    # Operation d'intersection geometrique\n",
    "    inter = gpd.overlay(lyr_cosia, gdf_parcelles, how='intersection')\n",
    "    if inter.empty:\n",
    "        continue\n",
    "    \n",
    "    # Calcul des surfaces et mapping\n",
    "    inter['S_INT'] = inter.geometry.area\n",
    "    \n",
    "    # Nettoyage du champ 'numero' spécifique à COSIA pour le mapping\n",
    "    inter['num_clean'] = inter['numero'].astype(str).str.replace('.', '', regex=False).str.strip()\n",
    "    \n",
    "    inter['CLASSE_NOM'] = inter['num_clean'].map(mapping_cosia).fillna(\"AUTRE\")\n",
    "    inter['PROBA_VAL'] = inter['num_clean'].map(mapping_proba).fillna(0.0)\n",
    "    \n",
    "    # Préparation du cumul pour la probabilité moyenne\n",
    "    inter['P_POND'] = inter['S_INT'] * inter['PROBA_VAL']\n",
    "\n",
    "    # Agregation statistique par unite fonciere\n",
    "    stats = inter.groupby(config['CHAMPS_ID'] + ['CLASSE_NOM']).agg({\n",
    "        'S_INT': 'sum',\n",
    "        'P_POND': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    # Pivotage des donnees\n",
    "    df_res = stats.pivot_table(index=config['CHAMPS_ID'], columns='CLASSE_NOM', values='S_INT', aggfunc='sum').fillna(0)\n",
    "    \n",
    "    for cl in classes_list:\n",
    "        if cl not in df_res.columns: df_res[cl] = 0.0\n",
    "\n",
    "    # Calcul des indicateurs de synthese (Dénominateur = Surface détectée)\n",
    "    df_res['SURF_TOTALE_M2'] = df_res[classes_list].sum(axis=1).round(3)\n",
    "    \n",
    "    # Sécurité pour éviter la division par zéro sur les parcelles vides\n",
    "    for cl in classes_list:\n",
    "        df_res[f\"{cl}_%\"] = 0.0\n",
    "        df_res.loc[df_res['SURF_TOTALE_M2'] > 0, f\"{cl}_%\"] = \\\n",
    "            ((df_res[cl] / df_res['SURF_TOTALE_M2']) * 100).round(3)\n",
    "\n",
    "    # Détermination de la dominante (Sécurisée contre les lignes à 0)\n",
    "    df_res['DOMINANTE'] = df_res[classes_list].idxmax(axis=1)\n",
    "    df_res.loc[df_res['SURF_TOTALE_M2'] == 0, 'DOMINANTE'] = \"AUTRE\"\n",
    "\n",
    "    # Calcul de la probabilité moyenne sur la classe dominante\n",
    "    proba_pivot = stats.pivot_table(index=config['CHAMPS_ID'], columns='CLASSE_NOM', values='P_POND', aggfunc='sum').fillna(0)\n",
    "    \n",
    "    def get_proba(row):\n",
    "        dom = row['DOMINANTE']\n",
    "        s_dom = row[dom]\n",
    "        try:\n",
    "            cumul = proba_pivot.loc[row.name, dom] if row.name in proba_pivot.index else 0\n",
    "            return round(cumul / s_dom, 3) if s_dom > 0 else 0\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "    df_res['PROBA_DOMINANTE'] = df_res.apply(get_proba, axis=1)\n",
    "\n",
    "    # --- 3. GENERATION DU CHAMP 'PL' (CONCATENATION SIMPLE) ---\n",
    "    df_res = df_res.reset_index()\n",
    "    df_res['PL'] = (\n",
    "        df_res['CODE_DEP'].astype(str).str.strip() + \n",
    "        df_res['CODE_COM'].astype(str).str.strip() + \n",
    "        df_res['SECTION'].astype(str).str.strip() + \n",
    "        df_res['NUMERO'].astype(str).str.strip()\n",
    "    )\n",
    "\n",
    "    # --- 4. EXPORTATION DES RESULTATS ---\n",
    "    output_csv = config['DOSSIER_CSV_COSIA'] / f\"STAT_PARCELLES_COSIA_{annee}.csv\"\n",
    "    cols_finales = ['PL'] + config['CHAMPS_ID'] + [f\"{cl}_%\" for cl in classes_list] + \\\n",
    "                   [\"DOMINANTE\", \"PROBA_DOMINANTE\", \"SURF_TOTALE_M2\"]\n",
    "    \n",
    "    df_res[cols_finales].to_csv(output_csv, sep=';', index=False, encoding='utf-8')\n",
    "    print(f\"SUCCES : Export COSIA millesime {annee} genere.\")\n",
    "\n",
    "print(\"TRAITEMENT COSIA TERMINE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daf611c0-52de-43ac-ae76-1e6bf936bdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> DÉMARRAGE DE LA SYNTHÈSE TEMPORELLE COSIA\n",
      "    > Lecture des millésimes : [2018, 2021, 2024]...\n",
      "    > Calcul de la décision finale et des indices de confiance...\n",
      "\n",
      ">>> SYNTHÈSE TERMINÉE. FICHIER GÉNÉRÉ : SYNTHESE_COSIA_FREQUENCE.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODULE : SYNTHÈSE TEMPORELLE COSIA\n",
    "# Description : Consolidation des millésimes COSIA et calcul du score de stabilité.\n",
    "# =============================================================================\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "print(\">>> DÉMARRAGE DE LA SYNTHÈSE TEMPORELLE COSIA\")\n",
    "\n",
    "# 1. SÉCURISATION DE LA CONFIGURATION\n",
    "# Récupération des chemins avec gestion des clés manquantes\n",
    "DIR_COSIA = config.get('DOSSIER_CSV_COSIA')\n",
    "# Sécurité : on cherche la clé dans config, sinon on utilise la variable globale\n",
    "DIR_SORTIE = config.get('DIR_SORTIE', globals().get('DIR_SORTIE'))\n",
    "CHAMPS_ID  = config.get('CHAMPS_ID')\n",
    "ANNEES_COSIA = sorted([int(a) for a in config.get('ANNEES_COSIA', [])])\n",
    "\n",
    "if not DIR_SORTIE:\n",
    "    raise ValueError(\"Erreur : DIR_SORTIE n'est pas défini dans la configuration.\")\n",
    "\n",
    "fichier_synthese = Path(DIR_SORTIE) / \"SYNTHESE_COSIA_FREQUENCE.csv\"\n",
    "base_globale = defaultdict(dict)\n",
    "\n",
    "# 2. CONSOLIDATION DES DONNÉES ANNUELLES\n",
    "print(f\"    > Lecture des millésimes : {ANNEES_COSIA}...\")\n",
    "for annee in ANNEES_COSIA:\n",
    "    chemin = Path(DIR_COSIA) / f\"STAT_PARCELLES_COSIA_{annee}.csv\"\n",
    "    if not chemin.exists():\n",
    "        print(f\"    ! Millésime {annee} non trouvé, passage à l'année suivante.\")\n",
    "        continue\n",
    "    \n",
    "    df = pd.read_csv(chemin, sep=';', encoding='utf-8')\n",
    "    for _, row in df.iterrows():\n",
    "        # Clé unique basée sur les identifiants pivots\n",
    "        cle = tuple(str(row[c]).strip() for c in CHAMPS_ID)\n",
    "        base_globale[cle][annee] = {\n",
    "            'DOM': row.get('DOMINANTE', 'AUTRE'),\n",
    "            'PROB': float(row.get('PROBA_DOMINANTE', 0.0))\n",
    "        }\n",
    "\n",
    "# 3. ARBITRAGE STATISTIQUE ET EXPORTATION\n",
    "print(\"    > Calcul de la décision finale et des indices de confiance...\")\n",
    "final_rows = []\n",
    "\n",
    "for cle, data in base_globale.items():\n",
    "    historique = [data[a]['DOM'] for a in ANNEES_COSIA if a in data]\n",
    "    if not historique: continue\n",
    "\n",
    "    # Calcul des fréquences d'apparition\n",
    "    counts = defaultdict(int)\n",
    "    for c in historique: counts[c] += 1\n",
    "    max_freq = max(counts.values())\n",
    "    gagnants = [cl for cl, nb in counts.items() if nb == max_freq]\n",
    "\n",
    "    # Application des règles d'arbitrage (Majorité vs Récence)\n",
    "    if len(gagnants) == 1:\n",
    "        elu, methode = gagnants[0], \"MAJORITE\"\n",
    "    else:\n",
    "        # Sélection de la classe la plus récente parmi les gagnants\n",
    "        elu = next((data[a]['DOM'] for a in reversed(ANNEES_COSIA) \n",
    "                    if a in data and data[a]['DOM'] in gagnants), \"AUTRE\")\n",
    "        methode = \"RECENT_SI_EGALITE\"\n",
    "\n",
    "    # Calcul des indicateurs de fiabilité\n",
    "    # SCORE_STABILITE : pourcentage de permanence de la classe élue\n",
    "    score_stabilite = round((counts[elu] / len(ANNEES_COSIA)) * 100, 1)\n",
    "    \n",
    "    # PROBA_MOYENNE : moyenne des probabilités sources pour la classe élue\n",
    "    probas_elu = [data[a]['PROB'] for a in data if a in data and data[a]['DOM'] == elu]\n",
    "    proba_moyenne = round(sum(probas_elu) / len(probas_elu), 3) if probas_elu else 0.0\n",
    "\n",
    "    # Reconstruction de l'identifiant standardisé PL\n",
    "    # Formatage : Dep(2) + Com(3) + Sec(2) + Num(4)\n",
    "    id_pl = \"\".join([cle[0].zfill(2), cle[2].zfill(3), cle[3].zfill(2), cle[4].zfill(4)])\n",
    "    \n",
    "    # Structuration de la ligne de sortie\n",
    "    res = {'PL': id_pl}\n",
    "    res.update({CHAMPS_ID[i]: cle[i] for i in range(len(CHAMPS_ID))})\n",
    "    \n",
    "    for a in ANNEES_COSIA:\n",
    "        res[f\"DOM_{a}\"] = data.get(a, {}).get('DOM', \"N/A\")\n",
    "        res[f\"PROB_{a}\"] = data.get(a, {}).get('PROB', 0.0)\n",
    "    \n",
    "    res.update({\n",
    "        'COSIA_CLASSE_FINALE': elu, \n",
    "        'SCORE_STABILITE': score_stabilite, \n",
    "        'PROBA_MOYENNE': proba_moyenne, \n",
    "        'METHODE_ARBITRAGE': methode\n",
    "    })\n",
    "    final_rows.append(res)\n",
    "\n",
    "# 4. EXPORTATION FINALE\n",
    "df_final = pd.DataFrame(final_rows)\n",
    "df_final.to_csv(fichier_synthese, sep=';', index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"\\n>>> SYNTHÈSE TERMINÉE. FICHIER GÉNÉRÉ : {fichier_synthese.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
